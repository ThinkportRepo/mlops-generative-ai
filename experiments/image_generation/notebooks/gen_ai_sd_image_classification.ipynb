{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering Pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "361f00dc40e5f09d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "598e6ca5c3e3c4e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import botocore\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "session = PipelineSession()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bae40f3ee589766f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload stable diffusion training data to S3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c636fa03cc82b8e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import create_bucket_if_not_exists\n",
    "\n",
    "\n",
    "# If uploading to a different folder, change this variable.\n",
    "local_training_dataset_folder = \"training_images\"\n",
    "\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "training_bucket = f\"stable-diffusion-jumpstart-{aws_region}-{account_id}\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    f\"jumpstart-cache-prod-{aws_region}\",\n",
    "    \"ai_services_assets/custom_labels/cl_jumpstart_ic_notebook_utils.py\",\n",
    "    \"utils.py\",\n",
    ")\n",
    "\n",
    "create_bucket_if_not_exists(training_bucket)\n",
    "train_s3_path = f\"s3://{training_bucket}/custom_cedar_apple_rust_stable_diffusion_dataset/\"\n",
    "# !aws s3 cp --recursive $local_training_dataset_folder $train_s3_path\n",
    "\n",
    "\n",
    "output_bucket = session.default_bucket()\n",
    "output_prefix = \"jumpstart-example-sd-training\"\n",
    "\n",
    "# needed for storing model artefacts\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/model\"\n",
    "\n",
    "s3_input_data_path = f\"s3://{output_bucket}/{output_prefix}/batch_input/\"\n",
    "s3_output_data_path = f\"s3://{output_bucket}/{output_prefix}/batch_output/\"\n",
    "\n",
    "# for idx in range(21):\n",
    "#     prompt_file_name = \"cedar_apple_rust\"\n",
    "#     s3.upload_file(f\"{prompt_file_name}.json\", output_bucket, f\"{output_prefix}/batch_input/{prompt_file_name}_{idx}.json\")    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0f3f9e5b25f3017"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Launch feature engineering pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4975f0bc74de8170"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/jumpstart-foundation-models/text2text-generation-Batch-Transform.html\n",
    "# https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform\n",
    "%time\n",
    "import json\n",
    "import uuid\n",
    "from time import strftime, gmtime\n",
    "\n",
    "import boto3\n",
    "\n",
    "from sagemaker import image_uris, script_uris, model_uris, Model\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.processing import ProcessingOutput, FeatureStoreOutput, ProcessingInput\n",
    "from sagemaker.sklearn import SKLearnProcessor\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "from sagemaker.workflow.retry import StepRetryPolicy, StepExceptionTypeEnum, SageMakerJobExceptionTypeEnum\n",
    "from sagemaker.workflow.steps import TransformStep, TrainingStep, ProcessingStep\n",
    "from sagemaker import hyperparameters\n",
    "from sagemaker.wrangler.processing import DataWranglerProcessor\n",
    "from sagemaker.workflow.notebook_job_step import NotebookJobStep\n",
    "\n",
    "\n",
    "train_model_id, train_model_version, train_scope = (\n",
    "    \"model-txt2img-stabilityai-stable-diffusion-v2-1-base\",\n",
    "    \"*\",\n",
    "    \"training\",\n",
    ")\n",
    "\n",
    "# Tested with ml.g4dn.2xlarge (16GB GPU memory) and ml.g5.2xlarge (24GB GPU memory) instances. Other instances may work as well.\n",
    "# If ml.g5.2xlarge instance type is available, please change the following instance type to speed up training.\n",
    "training_instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the training script. This contains all the necessary files including data processing, model training etc.\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values. This controls the duration of the training and the quality of the output.\n",
    "# If max_steps is too small, training will be fast but the the model will not be able to generate custom images for your usecase.\n",
    "# If max_steps is too large, training will be very slow.\n",
    "hyperparameters[\"max_steps\"] = \"200\"\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{train_model_id}-fine-tune\")\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "sd_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",  # Entry-point file in source_dir and present in train_source_uri.\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "\n",
    "train_step_args = sd_estimator.fit({\"training\": train_s3_path})\n",
    "\n",
    "# Define training step\n",
    "train_step = TrainingStep(name='sd-transfer-learning-step', step_args=train_step_args)\n",
    "\n",
    "inference_instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "inference_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "\n",
    "model = Model(\n",
    "  image_uri=inference_image_uri,\n",
    "  model_data=\"s3://sagemaker-eu-central-1-562760952310/jumpstart-example-sd-training/model/pipelines-klavescck34y-sd-transfer-learning-QAEuYiT8xt/output/model.tar.gz\",\n",
    "  sagemaker_session=session,\n",
    "  role=aws_role\n",
    ")\n",
    "\n",
    "\n",
    "# Define create model step\n",
    "model_step_args = model.create(instance_type=inference_instance_type)\n",
    "model_step = ModelStep(\n",
    "  name='sd-fine-tuned-model-step',\n",
    "  step_args=model_step_args\n",
    ")\n",
    "\n",
    "\n",
    "transformer =  Transformer(\n",
    "  model_name=model_step.properties.ModelName,\n",
    "  instance_type=inference_instance_type,\n",
    "  instance_count=4,\n",
    "  sagemaker_session=session,\n",
    "  output_path=s3_output_data_path,\n",
    "  strategy=\"SingleRecord\",\n",
    "  accept=\"application/json;jpeg\"\n",
    ")\n",
    "\n",
    "transform_args = transformer.transform(\n",
    "    s3_input_data_path, content_type=\"application/json\", split_type=\"Line\"\n",
    ")\n",
    "\n",
    "# Define transform step\n",
    "transform_step = TransformStep(name='sd-txt2image-batch-transform-step', step_args=transform_args, retry_policies=[\n",
    "    StepRetryPolicy(\n",
    "        exception_types=[\n",
    "            StepExceptionTypeEnum.SERVICE_FAULT,\n",
    "            StepExceptionTypeEnum.THROTTLING\n",
    "        ],\n",
    "        max_attempts=1\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(name='feature-engineering-pipeline',\n",
    "                    steps=[model_step, transform_step],\n",
    "                    sagemaker_session=session)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline.upsert(role_arn=aws_role, description='local pipeline for synthetic data generation with stable diffusion')\n",
    "\n",
    "# Start a pipeline execution\n",
    "execution = pipeline.start()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2604e1074e215df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "577b029d005303f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
